{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL and local paths\n",
    "url = \"https://github.com/rijpma/cathedrals/raw/master/dat/fullobs_sp.csv.gz\"  # Use the raw file URL\n",
    "local_folder = \"../Data\"\n",
    "local_gz_path = os.path.join(local_folder, \"fullobs_sp.csv.gz\")\n",
    "local_csv_path_1 = os.path.join(local_folder, \"fullobs_sp.csv\")\n",
    "\n",
    "# Step 1: Download the .gz file from the GitHub repository\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(local_gz_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {local_gz_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Unpack the .gz file to the CSV file\n",
    "with gzip.open(local_gz_path, 'rb') as f_in:\n",
    "    with open(local_csv_path_1, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    print(f\"Unpacked to {local_csv_path_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL and local paths\n",
    "url = \"https://github.com/rijpma/cathedrals/raw/refs/heads/master/dat/rurchurches_eb.csv\"  # Use the raw file URL\n",
    "local_folder = \"../Data\"\n",
    "local_csv_path_2 = os.path.join(local_folder, \"rurchurches_eb.csv\")\n",
    "\n",
    "# Step 1: Download the .gz file from the GitHub repository\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(local_csv_path_2, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {local_csv_path_2}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL and local paths\n",
    "url = \"https://github.com/rijpma/cathedrals/raw/refs/heads/master/dat/dynobs.csv\"  # Use the raw file URL\n",
    "local_folder = \"../Data\"\n",
    "local_csv_path_3 = os.path.join(local_folder, \"dynobs.csv\")\n",
    "\n",
    "# Step 1: Download the .gz file from the GitHub repository\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(local_csv_path_3, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {local_csv_path_3}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "fullobs_path = \"../Data/fullobs_sp.csv\"\n",
    "dynobs_path = \"../Data/dynobs.csv\"\n",
    "\n",
    "# Step 1: Load `fullobs` table and extract relevant columns\n",
    "fullobs = pd.read_csv(fullobs_path, low_memory=False)\n",
    "fullobs_subset = fullobs[['osmid', 'osmname', 'category', 'lat', 'lon']].drop_duplicates()\n",
    "\n",
    "# Step 2: Load `dynobs` table and extract the earliest construction year for each osmid\n",
    "dynobs = pd.read_csv(dynobs_path, low_memory=False)\n",
    "\n",
    "# Drop rows with missing 'osmid' or 'year' (essential for the analysis)\n",
    "dynobs = dynobs.dropna(subset=['osmid', 'year'])\n",
    "\n",
    "# Ensure 'osmid' is treated as a string for consistency\n",
    "dynobs['osmid'] = dynobs['osmid'].astype(str)\n",
    "\n",
    "# Group by 'osmid' and find the earliest construction event (minimum year)\n",
    "earliest_events = (\n",
    "    dynobs.groupby('osmid', as_index=False)\n",
    "    .agg(year_zero=('year', 'min'))  # Get the earliest year for each building\n",
    ")\n",
    "\n",
    "# Step 3: Merge metadata from `fullobs` with earliest construction years from `dynobs`\n",
    "consolidated_table = pd.merge(\n",
    "    fullobs_subset,\n",
    "    earliest_events,\n",
    "    on='osmid',\n",
    "    how='left'  # Ensure all `osmid` in `fullobs` are retained, even if no events in `dynobs`\n",
    ")\n",
    "\n",
    "# Step 4: Save the consolidated table to a CSV file\n",
    "consolidated_table.to_csv(\"../Data/consolidated_table.csv\", index=False)\n",
    "\n",
    "# Print the first few rows of the result for inspection\n",
    "print(consolidated_table.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where 'category' is 'parish' and 'year_zero' is greater than 1200\n",
    "consolidated_table.query(\"category == 'parish' or category == 'cathedral'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "file_path = \"../Data/rurchurches_eb.csv\"\n",
    "# Detect the file encoding\n",
    "with open(file_path, 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(result)  # Output will show detected encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Data/rurchurches_eb.csv\"\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "data = pd.read_csv(file_path, encoding='mac_roman', skiprows=1)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Forward-fill metadata\n",
    "data_ff = data.fillna(method='ffill')\n",
    "\n",
    "# Step 2: Filter rows where 'surface' is 'year'\n",
    "year_rows = data_ff[data_ff['surface'] == 'year'].copy()\n",
    "\n",
    "# Step 3: Extract year columns, accounting for the 6-column gap\n",
    "# The year data starts 6 columns after the 'surface' column\n",
    "start_idx = 11\n",
    "year_columns = data_ff.columns[start_idx:]\n",
    "\n",
    "# Create a DataFrame with osmid and year columns\n",
    "osmid_years = year_rows[['osmid'] + list(year_columns)]\n",
    "\n",
    "# Rename columns for clarity\n",
    "osmid_years.columns = ['osmid'] + [f'year_{i}' for i in range(1, osmid_years.shape[1])]\n",
    "\n",
    "# Step 4: Reshape year data into long format\n",
    "years_long = osmid_years.melt(id_vars='osmid', var_name='phase', value_name='year')\n",
    "years_long = years_long.dropna(subset=['year']).reset_index(drop=True)\n",
    "\n",
    "# Convert year column to numeric\n",
    "years_long['year'] = pd.to_numeric(years_long['year'], errors='coerce')\n",
    "\n",
    "# Step 5: Merge metadata with year data\n",
    "metadata_rows = data_ff[['osmid', 'osmname', 'category', 'lat', 'lon']].drop_duplicates(subset=['osmid'])\n",
    "merged = pd.merge(years_long, metadata_rows, on='osmid', how='left')\n",
    "\n",
    "# Step 6: Save the processed table\n",
    "output_path = \"../Data/rural_churches_processed.csv\"\n",
    "merged.to_csv(output_path, index=False)\n",
    "print(f\"Processed data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter rural data for `phase == 'year_1'`\n",
    "rural_year_1 = merged[merged['phase'] == 'year_1'].copy()\n",
    "\n",
    "# Step 2: Rename `year` column to `year_zero`\n",
    "rural_year_1 = rural_year_1.rename(columns={'year': 'year_zero'})\n",
    "\n",
    "# Step 3: Select relevant columns (matching consolidated structure)\n",
    "rural_year_1 = rural_year_1[['osmid', 'osmname', 'category', 'lat', 'lon', 'year_zero']]\n",
    "\n",
    "# Step 4: Combine the two dataframes\n",
    "combined_df = pd.concat([consolidated_table, rural_year_1], ignore_index=True)\n",
    "\n",
    "# Step 5: Save the combined dataframe\n",
    "output_path = \"../Data/churches_combined.csv\"\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "print(f\"Combined data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Filter the consolidated table\n",
    "filtered_table = combined_df.query(\"category == 'parish' or category == 'cathedral'\")\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    filtered_table,\n",
    "    geometry=gpd.points_from_xy(filtered_table['lon'], filtered_table['lat']),\n",
    "    crs=\"EPSG:4326\"  # Set the coordinate reference system to WGS 84\n",
    ")\n",
    "\n",
    "# Write to a GeoPackage\n",
    "output_path = \"../Output/churches_combined_gis.gpkg\"\n",
    "gdf.to_file(output_path, layer=\"filtered_table\", driver=\"GPKG\")\n",
    "\n",
    "print(f\"GeoPackage written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
